---
title: "Tweets"
author: "Minyoung Do"
date: "2/24/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tm)
library(grid)
library(wordcloud)
library(tidyverse)
library(tidytext)
library(topicmodels)
library(sentimentr)
library(broom)
library(purrr)
library(rsample)
```

```{r}
amy_tweets <- read_csv("amy_tweets.csv")
bernie_tweets <- read.csv("bernie_tweets.csv")
bloomberg_tweets <- read.csv("bloomberg_tweets.csv")
pete_tweets <- read.csv("pete_tweets.csv")
warren_tweets <- read.csv("warren_tweets.csv")
```

1. Valid time frame?

depends on our goal; are we looking at events and stuff?
grab as much tweets as we can over the biggest period as can be, and then randomly 
to create a synthetic population and then randomly select tweets... 
twitter is sensitive to events, having a noisy, non-random process.
can't fully generalize the result

iterate 

2. How to clean the data while keeping each tweet in a row?

unnest_token & nest_token
apply 

3. If we need to tokenize all the tweets to get the sentiment score, how do we merge them back by tweet id and average out all the scores of tokenized words in each tweet?

4. Polarization score
pick the candidate with the most extreme polarization
compare distribution shape to the most extreme candidate: relative polarization score



```{r}
set.seed(77)
amy_boot <- bootstraps(amy_tweets, times = 100)

summary(amy_boot)
```

```{r}

amy_text <- amy_tweets %>%
  select(text)
# unnest tokens first, and then create a subset, and then loop over all the documents...
# new dataset, get rid of stop words
# unnest output = bigram, separate bigram into unigrams
# kwic: quantdata pacakge, pass the corpus to the function and specify the keyword
# df[[i]]

# first attempt to clean the data
bloomberg_tweets_data <- bloomberg_tweets %>%
  select(created_at, text) %>%
  # how do I get rid of special characters without collapsing all the tweets into words?
  filter(!str_detect(text, "@\\w*")) %>%
  filter(!str_detect(text, "^[0-9]*$")) %>%
  filter(!str_detect(text, "http\\w*")) %>%
  filter(!str_detect(text, "amp")) %>%
  unnest_tokens(output = word, input = text) %>%
  anti_join(stop_words)
# use unnest_tokens and nest_tokens

amy_tweets_token = gsub("&amp", "", amy_text)
amy_tweets_token = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", amy_tweets_token)
amy_tweets_token = gsub("@\\w+", "", amy_tweets_token)
amy_tweets_token = gsub("[[:punct:]]", "", amy_tweets_token)
amy_tweets_token = gsub("[[:digit:]]", "", amy_tweets_token)
amy_tweets_token = gsub("http\\w+", "", amy_tweets_token)
amy_tweets_token = gsub("[ \t]{2,}", "", amy_tweets_token)
amy_tweets_token = gsub("^\\s+|\\s+$", "", amy_tweets_token)
# but how do I do this job without breaking texts into a character string?

# get sentences out of tweets
amy_sentence <- amy_tweets_2 %>%
  select(tweet_text)
# get emotion scores for each sentence
split_text <- get_sentences(amy_tweets$text)
emotion_amy <- emotion(split_text, drop.unused.emotions = T)
```

