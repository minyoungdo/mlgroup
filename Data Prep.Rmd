---
title: "Data Prep"
author: "Minyoung Do"
date: "2/22/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, cache = T, warning = F, message = F, error = F)
library(tidyverse)
library(rtweet)
library(tidytext)
library(twitteR)

api_key <- "pG8QAQeoEcV2KZyRBiscIqkME"
api_secret_key <- "gm6ywlCyIchQBCGv6pCgDOG4OGvdM986gDLf4YZ69lWQlvrwc9"
access_token <- "2963347195-eLlewwIf1X0jEr5mWg5rkyVvplalexhJXkPQpcp"
access_token_secret <- "A6jXnaE2GbeLAyCSOfA0ikgfgrWjZGJiDF0ddFZdLD33U"


## authenticate via web browser
token <- create_token(
  app = "minyoungdo",
  consumer_key = api_key,
  consumer_secret = api_secret_key,
  access_token = access_token,
  access_secret = access_token_secret)

get_token()

# importing tweets that contains "BernieSanders"
if (FALSE) {
## format datetime for one week ago
toDate <- format(Sys.time() - 60 * 60 * 24 * 7, "%Y%m%d%H%M")

## search 30day for up to 300 rstats tweets sent before the last week
rt <- search_30day("BernieSanders", n = 10000,
  env_name = "minyoung", fromDate = 202001240000, toDate = toDate)
}

```

```{r echo = T, results='hide'}
stall_rate_limit <- function(limit) {

  # Store the record of all the rate limits into rate
  rate = getCurRateLimitInfo()
  message("Checking Rate Limit")

  if(any(as.numeric(rate[,3]) == 0)) {

    # Get the locations of API Calls that are used up
    index = which(as.numeric(rate[,3]) == 0)

    # get the time till when rates limits Reset
    wait = as.POSIXct(min(rate[index,4]),     ## Reset times in the 4th col
                      origin = "1970-01-01",  ## Origin of Unix Time
                      tz = "US/Mountain")     ## Replace with your Timezone

    message(paste("Waiting until", wait,"for Godot to reset rate limit"))
    # Tell the computer to sleep until the rates reset
    Sys.sleep(difftime(wait, Sys.time(), units = "secs"))

    # Set J = to 0
    J = 0
    # Return J as a counter
    return(J)

    } else {

    # Count was off, Try again
    J = limit - 1
    return(J)

    }
}

followers <- rep(1, 2, 3, 4, 5)


callsMade = 0    ## This is your counter to count how many calls were made
limit = 180      ## the Limit of how many calls you can make
for(i in 1:length(followers)){

  # Check to see if you have exceeded your limit
  if(callsMade >= limit){

    # If you have exceeded your limit, wait and set calls made to 0 
    callsMade = stall_rate_limit(limit)

  }

  ### Execute your Code Here ... ###

  callsMade = callsMade + 1  # or however many calls you have made
}
```

# Tidying Twitter Data

```{r amy klobuchar}
amy_tweets <- search_tweets(q = "AmyKlobuchar", n = 18000, include_rts = FALSE)

# see comments in Part 1
amy_tweets_data <- amy_tweets %>%
  dplyr::select(created_at, text) %>%
  filter(!str_detect(text, "@\\w*")) %>%
  filter(!str_detect(text, "^[0-9]*$")) %>%
  filter(!str_detect(text, "http\\w*")) %>%
  filter(!str_detect(text, "amp")) %>%
  unnest_tokens(output = word, input = text) %>%
  anti_join(stop_words)

term_frequency_amy <- amy_tweets_data %>%
  count(word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  drop_na()

term_frequency_amy %>%
  top_n(20) %>%
ggplot(aes(x = reorder(word, -n), y = n, fill = word)) + 
  geom_bar(stat = "identity") +
  scale_x_reordered() +
  labs(title = "Most frequent words in Tweets Containing Amy Klobuchar",
       subtitle = "In Most Recent 18,000 Tweets Excluding ReTweets",
       x = NULL,
       y = "Word count") +
  coord_flip() +
  theme_minimal(base_size = rcfss::base_size * .65) +
  theme(legend.position = "none")

amy_sentiment <- amy_tweets_data %>%
  inner_join(get_sentiments("afinn")) %>%
  group_by(value) %>%
  count(value)

ggplot(data = amy_sentiment, aes(x = value, y = n, fill = value)) +
  geom_bar(stat = "identity") +
  theme(legend.position = "none") +
  labs(title = "Sentiment Score of Corpus",
       subtitle = "In Most Recent 18,000 Tweets Excluding ReTweets",
       x = "Sentiments of Tweets",
       y = "Number of Tweets")
```


```{r bernie}
bernie_tweets <- search_tweets(q = "BernieSanders", n = 18000, include_rts = FALSE)

bernie_tweets_data <- bernie_tweets %>%
  dplyr::select(created_at, text) %>%
  filter(!str_detect(text, "@\\w*")) %>%
  filter(!str_detect(text, "^[0-9]*$")) %>%
  filter(!str_detect(text, "http\\w*")) %>%
  filter(!str_detect(text, "amp")) %>%
  unnest_tokens(output = word, input = text) %>%
  anti_join(stop_words)

term_frequency_bernie <- bernie_tweets_data %>%
  count(word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  drop_na()

term_frequency_bernie %>%
  top_n(20) %>%
ggplot(aes(x = reorder(word, -n), y = n, fill = word)) + 
  geom_bar(stat = "identity") +
  scale_x_reordered() +
  labs(title = "Most frequent words in Tweets Containing Bernie Sanders",
       subtitle = "In Most Recent 18,000 Tweets Excluding ReTweets",
       x = NULL,
       y = "Word count") +
  coord_flip() +
  theme_minimal(base_size = rcfss::base_size * .65) +
  theme(legend.position = "none")

bernie_sentiment <- bernie_tweets_data %>%
  inner_join(get_sentiments("afinn")) %>%
  group_by(value) %>%
  count(value)

ggplot(data = bernie_sentiment, aes(x = value, y = n, fill = value)) +
  geom_bar(stat = "identity") +
  theme(legend.position = "none") +
  labs(title = "Sentiment Score of Corpus",
       subtitle = "In Most Recent 18,000 Tweets Excluding ReTweets",
       x = "Sentiments of Tweets",
       y = "Number of Tweets")
```


```{r warren}

warren_tweets <- search_tweets(q = "ElizabethWarren", n = 18000, include_rts = FALSE)

# see comments in Part 1
warren_tweets_data <- warren_tweets %>%
  dplyr::select(created_at, text) %>%
  filter(!str_detect(text, "@\\w*")) %>%
  filter(!str_detect(text, "^[0-9]*$")) %>%
  filter(!str_detect(text, "http\\w*")) %>%
  filter(!str_detect(text, "amp")) %>%
  unnest_tokens(output = word, input = text) %>%
  anti_join(stop_words)

term_frequency_warren <- warren_tweets_data %>%
  count(word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  drop_na()

term_frequency_warren %>%
  top_n(20) %>%
ggplot(aes(x = reorder(word, -n), y = n, fill = word)) + 
  geom_bar(stat = "identity") +
  scale_x_reordered() +
  labs(title = "Most frequent words in Tweets Containing Elizabeth Warren",
       subtitle = "In Most Recent 18,000 Tweets Excluding ReTweets",
       x = NULL,
       y = "Word count") +
  coord_flip() +
  theme_minimal(base_size = rcfss::base_size * .65) +
  theme(legend.position = "none")

warren_sentiment <- warren_tweets_data %>%
  inner_join(get_sentiments("afinn")) %>%
  group_by(value) %>%
  count(value)

ggplot(data = warren_sentiment, aes(x = value, y = n, fill = value)) +
  geom_bar(stat = "identity") +
  theme(legend.position = "none") +
  labs(title = "Sentiment Score of Corpus",
       subtitle = "In Most Recent 18,000 Tweets Excluding ReTweets",
       x = "Sentiments of Tweets",
       y = "Number of Tweets")
```


```{r pete}
pete_tweets <- search_tweets(q = "PeteButtigieg", n = 18000, include_rts = FALSE)

pete_tweets_data <- pete_tweets %>%
  dplyr::select(created_at, text) %>%
  filter(!str_detect(text, "@\\w*")) %>%
  filter(!str_detect(text, "^[0-9]*$")) %>%
  filter(!str_detect(text, "http\\w*")) %>%
  filter(!str_detect(text, "amp")) %>%
  unnest_tokens(output = word, input = text) %>%
  anti_join(stop_words)

term_frequency_pete <- pete_tweets_data %>%
  count(word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  drop_na()

term_frequency_pete %>%
  top_n(20) %>%
ggplot(aes(x = reorder(word, -n), y = n, fill = word)) + 
  geom_bar(stat = "identity") +
  scale_x_reordered() +
  labs(title = "Most frequent words in @realDonaldTrump Tweets",
       subtitle = "In Most Recent 1,000 Tweets Excluding ReTweets",
       x = NULL,
       y = "Word count") +
  coord_flip() +
  theme_minimal(base_size = rcfss::base_size * .65) +
  theme(legend.position = "none")

pete_sentiment <- pete_tweets_data %>%
  inner_join(get_sentiments("afinn")) %>%
  group_by(value) %>%
  count(value)

ggplot(data = pete_sentiment, aes(x = value, y = n, fill = value)) +
  geom_bar(stat = "identity") +
  theme(legend.position = "none") +
  labs(title = "Sentiment Score of Corpus",
       subtitle = "In Most Recent 18,000 Tweets Excluding ReTweets",
       x = "Sentiments of Tweets",
       y = "Number of Tweets")
```


```{r bloomberg}
bloomberg_tweets <- search_tweets(q = "MichaelBloomBerg", n = 18000, include_rts = FALSE)

bloomberg_tweets_data <- bloomberg_tweets %>%
  select(created_at, text) %>%
  filter(!str_detect(text, "@\\w*")) %>%
  filter(!str_detect(text, "^[0-9]*$")) %>%
  filter(!str_detect(text, "http\\w*")) %>%
  filter(!str_detect(text, "amp")) %>%
  unnest_tokens(output = word, input = text) %>%
  anti_join(stop_words)

term_frequency_bloomberg <- bloomberg_tweets_data %>%
  count(word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  drop_na()

term_frequency_bloomberg %>%
  top_n(20) %>%
ggplot(aes(x = reorder(word, -n), y = n, fill = word)) + 
  geom_bar(stat = "identity") +
  scale_x_reordered() +
  labs(title = "Most frequent words in Tweets Containing Michael Bloomberg",
       subtitle = "In Most Recent 18,000 Tweets Excluding ReTweets",
       x = NULL,
       y = "Word count") +
  coord_flip() +
  theme_minimal(base_size = rcfss::base_size * .65) +
  theme(legend.position = "none")

bloomberg_sentiment <- bloomberg_tweets_data %>%
  inner_join(get_sentiments("afinn")) %>%
  group_by(value) %>%
  count(value)

ggplot(data = bloomberg_sentiment, aes(x = value, y = n, fill = value)) +
  geom_bar(stat = "identity") +
  theme(legend.position = "none") +
  labs(title = "Sentiment Score of Corpus",
       subtitle = "In Most Recent 18,000 Tweets Excluding ReTweets",
       x = "Sentiments of Tweets",
       y = "Number of Tweets")
```


